\section{Prototypes, and exemplars, and corpora}
\label{sec:cogocl}

This paper deals with a morpho-syntactic alternation between two constructions that occurs only in a very specific type of measure noun phrase in German.
By \textit{alternation} I refer to a situation where two or more forms or constructions are available with no clear difference in acceptability, function, or meaning.
The study of constructional alternations has a long history in cognitively oriented corpus linguistics (for example, \citealp{BresnanEa2007,BresnanHay2010,BresnanFord2010,DivjakArppe2013,Gries2015,NessetJanda2010,Wulff2003}).
This area of research is based on the assumption that language is a probabilistic phenomenon \citep{Bresnan2007} where alternants are chosen neither deterministically nor fully at random.
Instead, multifactorial models are constructed which incorporate influencing factors from diverse levels, including contextual factors.
The estimation of the model coefficients quantifies the influence that the factors have on the probability that either alternant is chosen.
There are two fundamental issues to consider with respect to this tradition as a part of cognitive linguistics in the broad sense.
First, there is the question of whether corpus data do provide any insight into cognitive representations at all.
This question can and should be answered by testing how well corpus-derived models converge with or diverge from experimental findings, which provide more direct evidence of mental representations and processes but are often intrinsically narrower in scope.
Second and much related to the first question, the appropriate modelling of such results in cognitive linguistics is a key issue.

As for the first point, there has for a long time been an interest in correlating probabilistic generalisations extracted from corpus data with results from experimental work (for example, \citealp{ArppeJaervikivi2007,BresnanEa2007,BresnanFord2010,DivjakGries2008,DivjakEa2016,FordBresnan2013}).
This is often called a \textit{validation} of the corpus-derived findings, but \citet[303]{Divjak2016a} criticises this choice of words ``because it creates the impression that behavioral experimental data is inherently more valuable than textual data,'' citing \cite{TummersEa2005}, who state that a corpus is ``a sample of spontaneous language use that is (generally) realized by native speakers''.
However, as \citet[486--487]{Dabrowska2016} convincingly argues, this does not imply that we can in some way ``deduce mental representations from patterns of use,'' \ie\ from corpus data.%
\footnote{Epistemologically, I think she refers to acts of \textit{induction} rather than \textit{deduction}.}
It would be highly surprising if this were possible, and the same holds for experimental methods, albeit to a different degree.
Nobody assumes that we can inductively infer mental representations from experiments, which -- as opposed to corpus studies -- even allow for direct access to the cognitive agent and offer much better possibilities to control experimental conditions and nuisance variables.
Rather, under the standard approach, a theory of cognitive representation is pre-specified.
Then, predictions are derived from this theory \textit{before} the experiment or the corpus study is conducted in order to \textit{test} the theory.
While the same approach is by and large applicable to corpus data, I know discuss some relevant differences.

As mentioned above, the central question is whether usage data as found in corpora are truly predictive of speakers' and writers' cognitive representation of language and\slash or of their overall linguistic behaviour; and this is where the experimental validation (or, more neutrally \textit{corroboration}) comes into play.
An overview of this issue was given by \cite{NewmanSorensenduncan2015}, who enumerate a number of studies which show how corpus data and experimental data converge (such as \citealp{DurrantDoherty2010,BresnanEa2007,GriesWulff2005,GriesEa2005}) and a number of studies where the two types of data did not lead to diverging or only partially converging results (such as \citealp{ArppeJaervikivi2007,Dabrowska2014,Mollin2009}).
When researchers do not reach convergence, they mostly try to explain this by differentiating between the actual cognitive construct and what the pooled usage data as found in corpora represent, for example \citet[411]{Dabrowska2014}, who lists a long number of possible reasons to explain why subjects in her experiment diverged in their word association preferences from collocation measures extracted from corpora.
In some cases, researchers simply argue for a more adequate statistical analysis to increase the fit between corpus data and subjects' reactions in experiments, for example \cite{DivjakEa2016}, who show that generalised additive models (GAMs) are better suited than generalised linear models (GLMs) in the analysis of their reading-time experiment based on data extracted from previous corpus studies.
Much more optimistically compared to these publications dealing with possible frictions between usage and experimental data, \cite{StefanowitschFlach2016} recently proposed a straightforward positivist perspective of corpora as representing the input of an average adult speaker, thus licensing inferences from corpus data to cognitive representations under a ``corpus-as-input'' view.
They state that ``in this wider context, large, register-mixed corpora such as the British National Corpus [\ldots] may not be perfect models of the linguistic experience of adult speakers, but they are reasonably close to the input of an idealized average member of the relevant speech community'' \citep[104]{StefanowitschFlach2016}.%
\footnote{This, of course, raises the old question of how corpora should be composed to represent what they are intended to represent.
As points of reference for determining ``representativeness'', at least three major criteria have been used, namely the distribution of texts or text types in the output of all speakers of a language (production-based), the distribution of the relevance of texts or text types in the whole speech community (relevance-based), and the distribution of speakers' exposure to different texts or text types (perception-based).
See \citet{Biber1993}, \citet{MceneryEa2006}, \citet{Leech2007}, \citet{Hunston2008} for overviews from different perspectives.}
Clearly, no clear picture has yet emerged, which is not surprising given the vast number of cognitive constructs assumed at diverse levels (such as the level of words in collocation research and much more complex constructional levels in alternation research involving lexical, syntagmatic, and contextual factors), the problems of corpus composition, the operationalisations involved in making experiments, and in the choice of statistical tools.
While far from providing definitive solutions, my discussion in Section~\ref{sec:conclusion} will provide possible explanations for the goodness of fit between the corpus data and the experimental data reported in Sections~\ref{sec:corpusstudies} and~\ref{sec:experimental}, much in the spirit of \citet{Dabrowska2014}.

I now turn to the second issue, namely which type of cognitive representation research on alternations provides evidence for.
The typical approach in alternation research is to annotate a large number of corpus sentences with linguistic features and to model the probability of either variant being chosen given these features.
The idea is that a variant is chosen when the influencing features cumulatively assume \textit{prototypical} values for that variant.
I other words, a variant of prototype theory with features \citep{Rosch1978} suggests itself, and some researchers such as \cite{Gries2003}, \cite{NessetJanda2010}, or \cite{Schaefer2016c} indeed commit to prototype theory in alternation modelling.
Under prototype theory, category membership is defined by similarity to an ideal exemplar, which is usually identified with characterising features (see \citealp{Taylor2008} for an overview).
While prototype theory is well suited for modeling constructional choices, it is just one of at least two major similarity-based theories of classification, the most prominent other framework being exemplar theory (\citealp{MedinSchaffer1978,Hintzman1986}; see \citealp{StormsEa2000} for a comparison of the theories in different experimental settings).
Prototype theory and exemplar theory model essentially the same types of effects but differ significantly in whether they assume higher-level abstractions in the form of single maximally prototypical exemplars or their features (prototype theory) or assume that categories emerge through the storage of many exemplars and similarity classification on those exemplars (exemplar theory).
\hl{Parallel to Langacker here}

As \cite{Barsalou1990} already showed, however, prototype and exemplar theory model the same types of effects and are informationally equivalent.
Consequently, experiments which favour one theory over the other use procedural behaviour of subjects in experiments, for example the speed of category retrieval, as opposed to mere output data.
In very early experiments, \cite{PosnerKeele1968} showed, for example, that highly prototypical unseen exemplars were categorised more easily by subjects compared to less prototypical ones, even if these were included in the learning data.
This was (at the time) taken as evidence that subjects categorise by prototypes.
Since corpus data only show artefacts of production events and we have no experimental access to the speaker's or writer's performance, one should be sceptical whether corpus analysis alone could ever decide which theory of mental representation is more suitable (see also \citealp[22]{Gries2003} and the \citealp[486--487]{Dabrowska2016} quote above).

In cognitive science, it is mostly accepted that exemplar theories have greater explanatory power \citep[184]{Vanpaemel2016}, and that abstraction is only needed marginally, if at all.%
\footnote{The hard empirical evidence in favour of exemplar models is substantial.
  For example, in \cite{HahnEa2010}, the authors show that subjects even use exemplar similarity over abstract knowledge even when they are given very simple explicit rules.
  This is highly relevant because most other studies focus on the learning of implicit rule-based knowledge, which involves many auxiliary assumptions in actual experiments \citep[2]{HahnEa2010}.
On the other hand, there is evidence that neither theory is fully adequate to model humans' capabilities to form categories.
For example, \cite{ConawayKurtz2016} show that both prototype theory and exemplar theory fail to explain certain experimental results where subjects learn to generalise beyond the input in a way that cannot be explained by similarity.
}
Still, various attempts have been made over the past decades to settle the dispute between abstraction-based models (models with rules or prototypes) and exemplar models or to find models which unite the two extremes.
\cite{VanpaemelStorms2008,LeeVanpaemel2008} proposed the \textit{varying abstraction model} (VAM) which ``attempt[s] to balance economy and informativeness'' \citep[745]{LeeVanpaemel2008}, treating models with full abstraction (prototypes) and no abstraction at all (radical exemplar theory) as special cases of a model which allows for both abstraction and exemplar effects.
The mixture model of categorisation (MMC) by \cite{Rosseel2002} is a model with abstraction in the form of hierarchical clusters of exemplars, and these clusters of objects are characterised by a probability distribution over their features, and categorising new objects is a process of estimating the probability of this object belonging to one of the clusters.
\cite{GriffithsEa2009} go further and present a computational model based on the hierarchical Dirirchlet process which is able to chose the appropriate complexity of representation for a given category.
However, despite these (and more) attempts to reconcile or unite the two approaches while developing spelled-out mathematical models, \cite[183--184]{Vanpaemel2016} describes the state of affairs between adherents of neo-prototype theory (such as \citealp{MindaSmith2001,MindaSmith2002}) and exemplar theory as a stalemate.

%An aspect generally neglected in models of categorisation is how categorisation feeds exemplar generation and creativity (\ie category generation), two subjects of utmost importance to many linguists.
%In \cite{JernKemp2013}, a model is assumed where cognitive agents sample from learned distributions over both categories and exemplars to generate new ones.

In cognitive linguistics, \cite{DivjakArppe2013} is a very rare example of a paper where such issues are taken up.
Their corpus-based approach shows ``one way of systematically analyzing usage data as contained in corpora to yield a scheme, compatible with usage-based theories of language, by which the assumptions of both the prototype and exemplar theories can be operationalized'' \citep[267]{DivjakArppe2013}.
Their approach to implementing a varying abstraction model \citep[254--260]{DivjakArppe2013} is based on hierarchical clustering of annotated properties of sentences.
They hierarchically cluster sentences containing Russian verbs of trying.
Then, they single out one sentence from each cluster which scores the highest probability for any of the six \textit{try} verbs according to a polytomous regression model estimated on the same data.
The clusters are interpreted as intermediate-level exemplar-derived abstractions of typical contexts for these high-probability verbs (typically more than one cluster for each verb; \citealp[255--256]{DivjakArppe2013}).
The crucial difference between such data-driven corpus-based analyses and experiments in cognitive science (\citealp{DivjakArppe2013
} use \citealp{VerbeemenEa2007} as their reference) is that cognitive experimental research is based on experiments where subjects produce actual category assignments or similarity judgements, and in corpus studies, the categories and category membership is determined purely from existing data.
The experimental approach with reduced and\slash or artificial stimuli makes it much easier to examine very specific effects in the behaviour of the subjects.
While I am convinced that the results presented in \citet{DivjakArppe2013} are valid and important (especially given the previous and subsequent research the authors have conducted on the data, including experimental work as in \citealp{DivjakEa2016}), any data set can be clustered to yield a certain number of clusters.
Thus, the study does not in any way ensure that the clusters emerging from the data in any way correspond to any speaker's cognitive representation.%
\footnote{See, again, \citet[486--487]{Dabrowska2016} and \citet[22]{Gries2003}.}

However, the trade-off with experiments with highly simplified stimuli and very simple tasks is their lack of external validity (\ie their generalisability) and their high dependence on problematic operationalisations of constructs, control of confounding factors, etc. (in other words, dependence on construct and internal validity).
Tasks in cognitive science have been criticised exactly for their lack of external validity, for example by \citet{Murphy2003}.
From a linguistic perspective, it is remarkable in this context that \cite{VoorspoelsEa2011} consider their experimental task -- which is the assignment of typicality scores to nouns from the domains of \textit{animals} and \textit{artefacts} to categories like \textit{bird}, \textit{fish}, \textit{clothing}, or \textit{tools} -- a study of ``superordinate natural language categories, whereas most evidence supporting exemplar representations has been found in artificial categories of a more subordinate level'' \citep[1013]{VoorspoelsEa2011}.
Corpus linguists interested in probabilistic alternation modelling deal with much more complex high-level categories and use large and complex feature sets, especially in (morpho-)syntax.%
\footnote{Notice, however, that recently, approaches have emerged which solve at least some problems by abandoning linguistic high-level features altogether \citep{BaayenEa2016,RamscarPort2016}.
Clearly, they have not (or at least not yet) reached mainstream popularity.}
It is thus an advantage of much linguistic work on categorisation that it deals with complex and realistically produced data, because this greatly improves the \textit{external validity} of studies, \ie the generalisability of the findings.


I propose that the primary focus should be on determining which factors influence choices made by speakers.
As \cite[22]{Gries2003} put it with reference to classic prototype theory, ``even if the form of analysis does not translate into statements on mental representations, the high predictive power [...] shows that the cognitive factors underlying the choice of construction have been identified properly and weighted in accordance with their importance for actual usage.''
A major advancement since this early time has been the focus on which effects are at least more plausibly modelled as prototype\slash abstraction and exemplar effects, and how observed data fits the two approaches (see, for example, \citealp{DivjakArppe2013}).
This paper contributes this discussion.




% The present study is conducted within the general paradigm of cognitive corpus linguistics and includes a comparison of the corpus findings with results from two experiments.
% I assume a model of similarity-based classification in the form of Prototype Theory for modeling alternation.
% Protoype Theory has been used in alternation research in cognitively oriented corpus linguistics (see \citealp{DivjakArppe2013}; \citealp{Gries2003}).
% It is assumed that the variables annotated in a corpus study on an alternation phenomenon define prototypes for the alternants.
% The similarity of a given feature vector (in a concrete sentence where one of the alternants is used) influences to a large extent which alternant is chosen by speakers.
% This entails that a variant of Prototype Theory with features is assumed \citep{Rosch1978}, instead of the monolithic prototypes of earlier versions.
% Prototype Theory is well suited for modeling constructional choices but it is just one of several similarity-based theories of classification, the most prominent other framework being Exemplar Theory (\citealp{MedinSchaffer1978,Hintzman1986}; see \citealp{StormsEa2000} for a comparison of the theories in experimental settings).
% Prototype Theory and Exemplar Theory model essentially the same types of effects and differ mainly in whether they assume higher-level abstractions as part of the cognitive representation (Prototype Theory) or try to do without them (Exemplar Theory).
% I am sceptical that corpus analysis alone could ever decide which theory is more suitable, given that substantial doubt has been voiced whether even experimental methods are ultimately able to do so \citep{Barsalou1990}.%
% \footnote{In \cite{DivjakArppe2013}, it was shown using corpus data how both models form a converging picture.}
% Prototype Theory (with feature abstractions) is preferred here simply because it fits the established alternation modeling paradigm, which relies on features being weighted in statistical models.
% In Section~\ref{sec:analyses}, a prototype-theoretical parlance is therefore adopted.
% 
% In the remainder of the paper, I introduce the alternation phenomenon (Section~\ref{sec:descriptive}) and suggest a theory-driven set of factors influencing the alternation (Section~\ref{sec:analyses}).
% Then the corpus study is presented, including an appropriate statistical analysis (Section~\ref{sec:corpusstudies}).
% Two experiments which corroborate the corpus-based findings are then reported in Section~\ref{sec:experimental} before I sum up the paper in Section~\ref{sec:conclusion}.


