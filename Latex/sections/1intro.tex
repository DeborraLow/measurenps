\section{Prototypes, exemplars, and corpora in cognitive linguistics}
\label{sec:cogocl}

This paper deals with a morpho-syntactic alternation between two constructions which occurs only in a very specific type of measure noun phrase in German.
By \textit{alternation} I refer to a situation where two or more forms or constructions are available with no clear (but potentially a subtle) difference in acceptability, function, or meaning.
The study of lexical and constructional alternations has a long history in cognitively oriented corpus linguistics (for example, \citealp{BresnanEa2007,BresnanHay2010,BresnanFord2010,DivjakArppe2013,Gries2015,NessetJanda2010}).
This area of research is based on the assumption that language is a probabilistic phenomenon \citep{Bresnan2007} where alternants are chosen neither deterministically nor fully at random.
Instead, multifactorial models are constructed which incorporate influencing factors from diverse levels, including lexical and contextual factors.
The estimation of the model coefficients quantifies the influence that the factors have on the probability that either alternant is chosen.
There are two fundamental issues to consider with respect to this tradition as a part of cognitive linguistics in the broad sense.
First, there is the question of whether corpus data do provide any insight into cognitive representations at all.
This question can and should be answered by testing how well corpus-derived models converge with or diverge from experimental findings, which provide more direct evidence of mental representations and processes but are often intrinsically narrower in scope.
Second and closely related to the first question, the appropriate modelling of such results in cognitive linguistics (\ie\ the assumed underlying constructs) is a key issue.

Concerning the first point, there has for a long time been an interest in correlating probabilistic generalisations extracted from corpus data with results from experimental work (for example, \citealp{ArppeJaervikivi2007,BresnanEa2007,BresnanFord2010,DivjakGries2008,DivjakEa2016,FordBresnan2013}).
This is often called a \textit{validation} of the corpus-derived findings, but \citet[303]{Divjak2016a} criticises this choice of words ``because it creates the impression that behavioral experimental data is inherently more valuable than textual data'', citing \cite{TummersEa2005}, who state that a corpus is ``a sample of spontaneous language use that is (generally) realized by native speakers''.
See also \citet{Newman2011} for a very positive view of corpora as a source of data in cognitive linguistics in their own right.
However, as \citet[486--487]{Dabrowska2016} convincingly argues, this does not mean that we can in some way ``deduce mental representations from patterns of use'', \ie\ from corpus data.
It would be highly surprising if this were possible, and the same holds for experimental methods, albeit to a different degree.
Nobody assumes that we can inductively infer mental representations from experiments, which -- as opposed to corpus studies -- even allow for direct access to the cognitive agent and offer much better possibilities to control experimental conditions and nuisance variables.
Rather, a theory of cognitive representation is pre-specified.
Then, predictions are derived from this theory \textit{before} the experiment or the corpus study is conducted in order to \textit{test} the theory.
While the same approach is by and large applicable to corpus data, I now discuss some relevant differences.

As mentioned above, the central question is whether usage data as found in corpora are truly predictive of speakers' and writers' cognitive representation of language and\slash or of their overall linguistic behaviour; and this is where the experimental validation (or, more neutrally, \textit{corroboration}) comes into play.
An overview of this issue was given by \cite{NewmanSorensenduncan2015}, who enumerate a number of studies which show how corpus data and experimental data converge (such as \citealp{BresnanEa2007,DurrantDoherty2010,GriesWulff2005,GriesEa2005}) and a number of studies where the two types of data led to diverging or only partially converging results (such as \citealp{ArppeJaervikivi2007,Dabrowska2014,Mollin2009}).
When researchers do not achieve convergence, they mostly try to explain this by differentiating between the actual cognitive construct and what the pooled usage data as found in corpora represent.
For example, \citet[411]{Dabrowska2014} lists a long number of possible reasons to explain why subjects in her experiment diverged in their word association preferences from collocation measures extracted from corpora.
In some cases, researchers simply argue for a more adequate statistical analysis to increase the fit between corpus data and subjects' reactions in experiments, for example \cite{DivjakEa2016}, who show that generalised additive models (GAMs) are better suited than generalised linear models (GLMs) in the analysis of their reading-time experiment based on data extracted from previous corpus studies.
Much more optimistically compared to these publications dealing with possible frictions between usage and experimental data, \cite{StefanowitschFlach2016} recently proposed a straightforward positivist perspective of corpora as representing the input of an average adult speaker, thus licensing inferences from corpus data to cognitive representations learned from such data under a ``corpus-as-input'' view.
They state that ``in this wider context, large, register-mixed corpora such as the British National Corpus [\ldots] may not be perfect models of the linguistic experience of adult speakers, but they are reasonably close to the input of an idealized average member of the relevant speech community'' \citep[104]{StefanowitschFlach2016}.
In essence, this would entail that any generalisation extracted from the BNC could be assumed to have some kind of mental reality, which is at least doubtful.

Obviously, no clear picture has emerged yet, which is not surprising given the vast number of cognitive constructs assumed at diverse levels (such as the level of words in collocation research and much more complex constructional levels in alternation research involving lexical, syntagmatic, and contextual factors), the problems of corpus composition, the operationalisations involved in making experiments, and the choice of statistical tools.
While far from providing definitive solutions, my discussion in Section~\ref{sec:conclusion} will provide possible explanations for the goodness of fit between the corpus data and the experimental data reported in Sections~\ref{sec:corpusstudies} and~\ref{sec:experimental}, much in the spirit of \citet{Dabrowska2014}.

I now turn to the second issue, namely which type of cognitive representation research on alternations provides evidence for.
The typical approach in alternation research is to annotate a large number of corpus sentences with linguistic features and to model the probability of the variants being chosen given these features.
The idea is that a variant is chosen when the influencing features cumulatively assume typical values for that variant.
In other words, a variant of prototype theory with features \citep{Rosch1978} is the appropriate model, also because the features used to feed the model are more often than not abstract high-level linguistic features. 
Some researchers such as \cite{Gries2003}, \cite{NessetJanda2010}, or \cite{Schaefer2016c} indeed commit to prototype theory in alternation modelling.
Under prototype theory, category membership is defined by similarity to an ideal exemplar, which is usually identified with characterising features (see \citealp{Taylor2008} for an overview).
While prototype theory is well suited for modeling constructional choices, it is just one of at least two major similarity-based theories of classification, the most prominent other framework being exemplar theory (\citealp{MedinSchaffer1978,Hintzman1986}).
Prototype theory and exemplar theory model essentially the same types of effects when only output data are considered.
Corpus data (representing purely output data, usually from many speakers) might serve as evidence that some form of similarity-based classification is appropriate.
However, the theories differ significantly in whether they assume higher-level abstractions in the form of single maximally prototypical exemplars or their features (prototype theory) or assume that categories emerge through the storage of many exemplars and similarity classification on those exemplars (exemplar theory).
Similarity-based categorisation is seen as the central and conceptually sufficient approach to linguistic categorisation by many cognitive linguists.
For example, see \cite{Taylor2003} for a comprehensive treatment in terms of prototype theory and \cite{Taylor2012} for a detailed discussion of an exemplar-based approach.
In the influential framework of cognitive grammar \citep{Langacker1987}, prototypes (which, as should be remembered, represent abstractions already in cognitive science) are literally taken as prototypical exemplars, and there is an additional level of fully discrete abstractions in the form of schemas.
Schemas are characterised by the properties common to all members of the category, whereas a prototypical category member might have very specific additional properties not at all shared by all or even most members \citep[371-375]{Langacker1987}.
The prototype can serve as a reference point when classifying new objects which do not share all properties of the schema, but this would (if repeated) lead to the creation of an even more abstract (hierarchically higher and less specific) schema which describes the new member and the ones belonging to the previous schema.
As pointed out by \citet[136--137]{Langacker1987}, schemas and prototypes thus fulfil different roles and can be assumed to co-exist.
A strict exemplar view of language is incompatible, as far as I can see, with Langacker's view of schemas, but any theory of categorisation that allows for at least some kind of abstraction, is not in principled contradiction with it.
In the remainder of this paper, I do not use schemas in my descriptions of the relevant categories, mostly because the aspect of similarity and fuzzy classification is central to my point, and a formulation in terms of schemas would bring about an unnecessarily high degree of abstraction (see \citealp[70--71]{Taylor2003} for a parallel argument).
However, I fully describe the abstract features of the prototypes.

Turning back to the prototype vs.\ exemplar debate, \cite{Barsalou1990} already showed that prototype and exemplar theory model the same types of effects and are informationally equivalent.
Consequently, experiments which favour one theory over the other use procedural behaviour of subjects in experiments, for example the speed of category retrieval, as opposed to mere output data.
In very early experiments, \cite{PosnerKeele1968} showed, for example, that highly prototypical unseen exemplars were categorised more easily by subjects compared to less prototypical ones which had been included in the learning data.
This was (at the time) taken as evidence that subjects categorise by prototypes.%
\footnote{See \citet{StormsEa2000} for a comparison of the theories in different experimental settings.}
Since corpus data only show artefacts of production events and we have no experimental access to the speaker's or writer's performance and their actual similarity judgements, one should be sceptical whether corpus analysis alone could ever decide which theory of mental representation is more suitable (see also \citealp[22]{Gries2003} and the \citealp[486--487]{Dabrowska2016} quote above).
However, as I will show, some effects are more naturally analysed as prototype effects while others are almost necessarily exemplar effects.

In cognitive science, it is mostly accepted that exemplar theories have greater explanatory power \citep[184]{Vanpaemel2016}, and that abstraction is only needed marginally, if at all.
Still, various attempts have been made over the past decades to settle the dispute between abstraction-based models (models with rules or prototypes) and exemplar models or to find models which unite the two extremes.
\cite{VanpaemelStorms2008} and \citet{LeeVanpaemel2008} proposed the \textit{varying abstraction model} (VAM) which ``attempt[s] to balance economy and informativeness'' \citep[745]{LeeVanpaemel2008}, treating models with full abstraction (radical prototype theory) and no abstraction at all (radical exemplar theory) as special cases of a model which allows for both abstraction and exemplar effects.
The mixture model of categorisation (MMC) by \cite{Rosseel2002} is a model with abstraction in the form of hierarchical clusters of exemplars, and these clusters of objects are characterised by a probability distribution over their features, and categorising new objects is a process of estimating the probability of this object belonging to one of the clusters.
\cite{GriffithsEa2009} go further and present a computational model which is able to choose the appropriate complexity of representation for a given category.
However, despite these (and more) attempts to reconcile or unite the two approaches while developing spelled-out mathematical models, \cite[183--184]{Vanpaemel2016} describes the state of affairs between adherents of neo-prototype theory (such as \citealp{MindaSmith2001,MindaSmith2002}) and exemplar theory as a stalemate.

In cognitive linguistics, \cite{DivjakArppe2013} is a very rare example of a paper where such issues are taken up with reference to the current research in cognitive science.
Their corpus-based approach shows ``one way of systematically analyzing usage data as contained in corpora to yield a scheme, compatible with usage-based theories of language, by which the assumptions of both the prototype and exemplar theories can be operationalized'' \citep[267]{DivjakArppe2013}.
Their approach to implementing a varying abstraction model \citep[254--260]{DivjakArppe2013} is based on hierarchical clustering of annotated properties of sentences.
They cluster sentences containing Russian verbs of trying.
Then, they single out the one sentence from each cluster which scores the highest probability for any of the six \textit{try} verbs according to a polytomous regression model estimated on the same data.
The clusters are interpreted as intermediate-level exemplar-derived abstractions of typical contexts for these high-probability verbs (typically more than one cluster for each verb; \citealp[255--256]{DivjakArppe2013}).
The crucial difference between such data-driven corpus-based analyses and experiments in cognitive science (\citealp{DivjakArppe2013} use \citealp{VerbeemenEa2007} as their reference) is that cognitive research is based on experiments where subjects produce actual category assignments or similarity judgements, and in corpus studies, the categories and category membership are determined purely from existing data.
The experimental approach with reduced and\slash or artificial stimuli makes it much easier to examine very specific effects in the behaviour of the subjects.
While I am convinced that the results presented in \citet{DivjakArppe2013} are valid and important (especially given the previous and subsequent research the authors have conducted on the data, including experimental work presented in \citealp{DivjakEa2016}), any data set can be clustered to yield a certain number of clusters.
Thus, the study does not ensure that the clusters emerging from the data correspond to any speaker's cognitive representation.%
\footnote{See, again, \citet[486--487]{Dabrowska2016} and \citet[22]{Gries2003}.}

On the other hand, the trade-off one has to accept when doing experiments with highly simplified stimuli and very simple tasks is their lower \textit{external validity} (\ie\ their lower degree of generalisability) and their high dependence on potentially problematic operationalisations of constructs, control of confounding factors in the face of a limited number of available subjects, etc. (in other words, critical dependence on \textit{construct validity} and \textit{internal validity}).%
\footnote{An accessible overview of the different types of validity can be found in Chapter~1 of \citet{MaxwellDelaney2004}.}
Tasks in cognitive science have been criticised exactly for their lack of external validity, for example by \citet{Murphy2003}.
From a linguistic perspective, it is remarkable in this context that \cite{VoorspoelsEa2011} consider their experimental task -- which is the assignment of typicality scores to nouns from the domains of \textit{animals} and \textit{artefacts} to categories like \textit{bird}, \textit{fish}, \textit{clothing}, or \textit{tools} -- a study of ``superordinate natural language categories, whereas most evidence supporting exemplar representations has been found in artificial categories of a more subordinate level'' \citep[1013]{VoorspoelsEa2011}.
Corpus linguists interested in probabilistic alternation modelling deal with much more complex high-level categories and use large and complex feature sets, especially in (morpho-)syntax.%
\footnote{Notice, however, that recently, approaches have emerged which solve at least some problems by abandoning linguistic high-level features altogether \citep{BaayenEa2016,RamscarPort2016}.
Clearly, they have not (or at least not yet) reached mainstream popularity, and it remains to be seen how well they perform on a broader range of questions.}
It is thus an advantage of much linguistic work on categorisation that it deals with complex and realistically produced data because this greatly improves the external validity of studies, although by sacrificing some construct validity.
An ideal contribution by cognitive corpus linguists to the research on (levels of) category abstraction in the human mind would thus be to provide analyses which have great external validity and complexity while carefully making sure that (and determining to what extent) these finding correlate with reactions from cognitive agents under more controlled experimental conditions, which increases the construct validity.

This paper contributes to this endeavour in many ways.
After a thorough description of the alternation under examination, I discuss potential influencing factors comprising high-level abstract semantic generalisations as well as exemplar-similarity and item-specific effects in Section~\ref{sec:germanmeasurenps}.
I will argue that
there are lemma-specific and exemplar effects but also generalisations at the level of the construction as well as generalisations overlaying the lemma-specific effects, leading to a complex hierarchical structure.
In Section~\ref{sec:corpusstudies}, I present a corpus study and report a true multilevel generalised linear model with the appropriate hierarchical structure for the hypothesised effects.
In Section~\ref{sec:experimental}, I test the predictions of the corpus-based model in two experimental paradigms (forced-choice and self-paced reading), showing that they indeed converge, albeit with low effect strength.
In Section~\ref{sec:conclusion}, I interpret the findings in the light of the issues of cognitive representations and corpus data and the convergence of usage data and experimental data.
