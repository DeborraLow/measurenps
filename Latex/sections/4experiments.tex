\section{Experiments}
\label{sec:experimental}

\subsection{Experiment 1: forced-choice}
\label{sec:exp:fc}

In the two experiments reported in this section, I use probabilities for the alternating constructions calculated for attested material, and I correlate these probabilities with the participants' reactions.
Thus, a direct link can be established between output material found in corpora and the behaviour of linguistic agents (see also Section~\ref{sec:cogocl}).
Both experiments use sentences containing attested MNPs from the corpus sample (embedded into simplified sentences) as stimuli.
Also, the probabilities that the corpus-based model assigns to the two alternants in these sentences are used as the main regressor in both studies.

The first experiment tests preferences for constructions explicitly.
\cite{FordBresnan2013} use the \textit{split-100} task in which participants have to distribute 100 points between the alternatives, assigning more points to more natural sounding alternative.
In essence, participants distribute a probability mass between two alternants, which is intended to produce more subtle results compared to a two-alternative forced-choice task such as in \cite{Rosenbach2013}, where participants have to choose one of two alternants.
The split-100 paradigm has been criticised in \cite{ArppeJaervikivi2007}.
The criticism was reiterated in \cite{DivjakEa2016}, where they use a forced-choice task.
In \cite{VerhoevenTemme2017}, it was shown that results from forced-choice and split-100 experiments mostly converge (with some numeric intricacies related to the non-linear distribution of preferences for alternants).
I present a forced-choice experiment in this Section instead of a split-100 experiment not only because of some researchers' critical attitude towards the split-100 task, but also because in a dry run of the experiment, participants complained about the unnaturalness of distributing a probability mass across two alternants and tended to produce ratings of 0 and 100 (and to a lesser degree 50).
Participants had to choose between two sentences that differed only in that one contained the \NACa\, and the other one contained the \PGCa.
The analysis compares the probabilities assigned to the stimuli by the corpus-derived model with the frequency with which participants chose the alternants for the same stimuli.

There were 24 participants (native speakers of German without reading or writing disabilities) aged 19 to 30 living permanently in Berlin, who were recruited from introductory linguistics courses at Freie Universität Berlin.
Although the experiment was conducted in the last four weeks of their first semester, participants had no deeper explicit knowledge of linguistics, grammar, or experimental methods.
None of them had ever participated in a forced-choice experiment before.
Participation was voluntary but participants received credit in partial fulfillment of course requirements.

As stimuli, attested MNPs from the corpus study were used, but the sentences were radically simplified to avoid influences from contextual nuisance factors as much as possible.
The approach is also justified because according to the theoretical assessment in Section~\ref{sec:analyses}, the choice of alternants depends mostly on a very local constructional context.
I sampled 16 MNPs from the concordance and made sure that the simplifications and normalisations did not affect any of the regressors used in the corpus study.
In the simplified sentences, the case, number, etc. of the MNP remained the same as in the attested sentence, as did the choice of lexical material within the MNP.
Eight sentences contained masculine or neuter kind nouns, and the other eight contained feminine kind nouns.
Furthermore, in each of the masculine\slash neuter and feminine groups, four sentences originally containing the \NACa\ and four sentences originally containing the \PGCa\ were chosen.
More precisely, the sentences were sampled as \textit{highly typical examples} of \PGCa\ (high probability assigned by GLMM) and \NACa\ (low probability assigned by GLMM), respectively.%
\footnote{Remember from Section~\ref{sec:corpusstudies} that the model predicts the probability that the \PGCa\ is chosen over the \NACa.}
High and low probabilities were defined as the top and bottom 20\% of all probabilities assigned by the GLMM.
Lemmas and feature combinations were made unique within each group whenever possible.
The design is summarised in Table \ref{tab:experiment1:design}.

\begin{table}
  \centering
  \begin{tabular}[h]{lll}
     & Masculine\slash Neuter & Feminine \\
     \midrule
     \textbf{high prob.\ for PGC\Subsf{adj}} & 4 sentences & 4 sentences \\
     \textbf{low prob.\ for PGC\Subsf{adj}} & 4 sentences & 4 sentences \\
  \end{tabular}
  \caption{The four groups of sentences chosen as stimuli; in each group of four sentences, combinations of important factor values were made unique whenever possible}
  \label{tab:experiment1:design}
\end{table}

The final pairs of stimuli were the sentence containing the attested and preferred alternant (according to the corpus GLMM) on the one hand and a modified version containing the dispreferred alternant on the other hand.
They were presented next to each other, and a 20 second time limit for each choice was set.%
\footnote{No participant ever exceeded the time limit.}
The position on the screen (left\slash right) and the order of sentences were randomised for each participant.
As fillers, 23 pairs of sentences exemplifying similar but unrelated alternation phenomena from German morpho-syntax were used.
Thus, participants saw 39 pairs of sentences and 78 sentences in total.
They were instructed to select from each pair of sentences the one that seemed more natural to them in the sense that they would use it rather than the other one.
The experiment was conducted using \textit{PsychoPy} \citep{Peirce2007}.

\begin{figure}[htbp!]
\centering
\includegraphics[width=0.5\textwidth]{../R/output/fc_effects}
\caption{Effect plot for the multilevel logistic regression in the forced-choice experiment: predictability of participants' choices using the probabilities derived from the corpus-based GLMM}
\label{fig:afc:effects}
\end{figure}

Then, a multilevel logistic regression was specified with the probability of the \PGCa\ predicted for each sentence by the corpus-based GLMM as the only fixed effect \textit{Modelprediction}.%
\footnote{The document-level variables \textit{Badness} and \textit{Genitives} were set to 0, which is the mean for z-transformed variables.}
A random intercept and slope were added for the individual sentence (item) in order to catch idiosyncrasies of single sentences.
Coefficients were estimated with Maximum Likelihood Estimation (\textit{lmer} function from \textit{lme4}).
The number of observations was \textit{n=}384.

\begin{figure}[htbp!]
  \centering
  \includegraphics[width=0.5\textwidth]{../R/output/fc_proportions}
  \caption{Spineplot of the proportion of responses plotted against the predictions from the corpus-based model in the forced-choice experiment}
  \label{fig:spines}
\end{figure}

A good amount of the variance can be accounted for by idiosyncrasies of single sentences ($\sigma_{\text{Item}}=1.217$).%
Also, among participants, there are clearly different preferences ($\sigma_{\text{Participant}}=0.412$).
On the extreme ends, one participant chose the \PGCa\ in 13 of 16 cases, and two participants only chose it in 5 of 16 cases.
The regressor \textit{Modelprediction} achieves $\mpPB=0.003$ (1,000 replications) and is estimated at 4.389 relative to an intercept of -1.270.
The confidence interval from a parametric bootstrap (1,000 replications, percentile method) for the regressor is acceptable but slightly large with a lower bound of 1.788 and an upper bound of 6.599.
The pseudo-coefficients of determination are $R^2_{m}=0.185$ and $R^2_{c}=0.455$, which means that roughly 19\% of the variance in the data can be explained by considering only the predictions from the corpus-based GLMM.
This is a weak result for the fixed effects part, but clearly worthy of mention.
The effect display for the single fixed regressor \textit{Modelprediction} is given in Figure \ref{fig:afc:effects}.
The higher the probability of the \PGCa\ predicted from usage data, the more often participants chose the \PGCa\ alternant in the forced-choice task.
A closer look at the results in the form of the spineplot in Figure~\ref{fig:spines} shows, however, that it was likely idiosyncracies in the sentences with a model prediction between 0.5 and 0.6 which spoiled an otherwise much better correlation.
Clearly, a revised and improved experiment might lead to a much better fit in future research.

In principle, a per-stimulus random effect for \textit{Modelprediction} could also remedy the problem with individual stimuli at least partially.
Therefore, a model with random slopes for \textit{Modelprediction} and both random effects (\textit{Participant} and \textit{Item}) was specified and estimated.
The random slope for participants was added to comply with \citet[257]{BarrEa2013} who predict \textit{catastrophically high Type I error rates} for experimental designs with within-subject manipulations if random effects structures are not kept maximal.
The coefficient of the fixed effect changed noticeably but not enough to change the interpretation (5.408 relative to an intercept of -1.304), and the marginal $R^2_m$ rises to 0.213 ($R^2_c=0.488$).
In line with expectations, the standard deviation in the random slopes for \textit{Item} is high at 5.996.
However, the covariance parameters were estimated at -1.0, which is a clear sign that the variance-covariance matrix could not be estimated successfully.
The same was true for models with only \textit{Item} and \textit{Participant} random slopes.
This is exactly the kind of model overparametrisation criticised in \citet{BatesEa2015a} and \citet{MatuschekEa2017}.
The available data is simply insufficient to estimate the parameters of the more complex model with varying slopes.%
\footnote{\citet[1]{BatesEa2015a} state: ``We show that failure to converge typically is not due to a suboptimal estimation algorithm, but is a consequence of attempting to fit a model that is too complex to be properly supported by the data, irrespective of whether estimation is based on maximum likelihood or on Bayesian hierarchical modeling with uninformative or weakly informative priors.
Importantly, even under convergence, overparameterization may lead to uninterpretable models.''
}

In summary, the forced-choice experiment succeeded in corroborating the results from the corpus study in as much as the preferences extracted from usage data correspond to native speakers' choices, but the correlation is weak, likely due to problems with individual test items and\slash or too few data.


\subsection{Experiment 2: self-paced reading}
\label{sec:exp:spr}

The second experiment tests preferences more implicitly.
It is expected that reading less typical alternants (the one assigned a low probability by the corpus-derived model) in a given context and with given lexical material incurs a processing overhead for the reader (\citealp{Kaiser2013}).
In this section, a self-paced reading experiment is therefore presented.
In a very similar fashion, \cite{DivjakEa2016} apply the self-paced reading paradigm in the validation of corpus-based models.
The analysis compares the corpus-derived probabilities with potential lags in reading time for sentences with the preferred and the non-preferred constructions.

Concretely, the exact same stimuli as in the forced-choice experiments were used.
Each participant read both the 16 sentences with the alternant predicted by the corpus model and the 16 modified sentences with the alternant that the corpus model did not predict.%
\footnote{Notice that lemmas and their frequencies as well as lemma classes are included as regressors in the corpus-based GLMM, and there was consequently no additional controlling of lemma frequencies, etc.}
To minimise repetition effects, the stimuli for each participant were separated into two blocks of 16 targets and 33 fillers per block.
In the experiment, participants first read all sentences from the first block, then all sentences from the second block.
From each target sentence pair, one sentence was assigned to the first block and the other sentence to the other block.
The assignment of members of the individual sentence pairs to the blocks was randomised for each participant individually, as was the order within each block.
The sentences from each pair of alternants were kept as far apart as possible.
The fillers also came in pairs such that the second block exclusively contained sentences to which participants had been exposed in the first block in slightly modified form.
In total, each participant read 98 sentences.
After each sentence, participants had to answer simple (non-metalinguistic) yes-no questions about the previous sentence as distractors.
The distractor questions were different between the first and the second blocks.
There were 38 participants recruited in exactly the same manner as for the experiment reported in Section~\ref{sec:exp:fc}.
None of them had ever participated in any kind of reading experiment, and none of them took part in the first experiment.
The experiment was conducted using \textit{PsychoPy}.

The reading times were residualised per speaker based on the reading times of all words (not just the targets) by that speaker.
The adjective and the kind noun (\ie\ the constituents bearing the critical case markers) were used as the target region, for instance the bracketed words in the example \textit{zwei Gläser} [\textit{spru\-deln\-des Wasser}] `two glasses of sparkling water'.
Outliers farther than 2 inter-quartile ranges from the mean logarithmised residualised reading time were removed (64 data points), resulting in a total number of \textit{n=}1,152 observations.
An LMM was specified with the logarithmised residual reading times as the response variable.

The probabilities derived from the corpus GLMM (\textit{Modelprediction}) were added as the main regressor of interest.
It should be remembered that the corpus GLMM predicts the probability of the \PGCa.
As a consequence, the higher the GLMM prediction is, the more typical the sentence is for containing the \PGCa.
It is therefore expected that reading times are higher when the value of \textit{Modelprediction} is higher but the sentence contains the \NACa.
However, when the sentence contains the \PGCa, reading times should be lower when \textit{Modelprediction} is higher.
To account for this, an interaction between \textit{Modelprediction} and \textit{Construction} (levels \textit{PGCadj} and \textit{NACadj}) was added to the model.

Furthermore, the position (1--98) of the sentence in the individual experiment (\textit{Position}) was included as a fixed effect to control for the usual increase in reading speed during an experiment run.
Random intercepts were specified for \textit{Participant} and \textit{Item} (the 16 sentence pairs are one \textit{Item} each).%
\footnote{Again, all attempts to include random slopes resulted in the variance-covariance matrix not being properly estimated (1 or -1 covariance parameters).}

\begin{table}
  \centering
  \begin{tabular}{lrrrc}
    Regressor & \multicolumn{1}{r}{Coefficient} & \multicolumn{1}{r}{CI low} & \multicolumn{1}{r}{CI high} & \multicolumn{1}{r}{CI excludes 0} \\ \midrule
    Construction=PGCadj                 &  0.054 &  0.012 &  0.095 &  *  \\ 
    Modelprediction                     & -0.003 & -0.113 &  0.110 &     \\ 
    Position                            & -0.005 & -0.005 &  0.004 &     \\ 
    Construction=PGCadj:Modelprediction & -0.125 & -0.234 & -0.023 &  *  \\ 
  \end{tabular}
  \caption{Fixed effect coefficient table for the LMM used to analyse the self-paced reading experiment; the intercept is 0.829}
  \label{tab:exp:spr}
\end{table}

Table~\ref{tab:exp:spr} shows the coefficient estimates with a 95\% parametric bootstrap confidence interval (1,000 replications, percentile method).
The standard deviation of the participant intercepts is $\sigma_{\text{Participant}}=0.079$ and of the item intercepts $\sigma_{\text{Item}}=0.037$.
Comparing the full model to a model without the main regressor \textit{Modelprediction} (and consequently also without the interaction with \textit{Construction}) in a PB test gives $\mpPB=0.036$.
The pseudo-coefficients of determination are $R^2_m=0.239$ and $R^2_c=0.346$.
There is thus some inter-subject variation and a noteworthy correlation between the reactions of the subjects and the corpus-derived probabilities.

\begin{figure}[htbp!]
\centering
\includegraphics[width=0.5\textwidth]{../R/output/spr_effects}
\caption{Effect plot for the LMM in the self-paced reading experiment: modeling participants' residualised log reading times on the probabilities given by the corpus-based GLMM}
\label{fig:spr:effects}
\end{figure}

The effect plot for the interaction of interest is shown in Figure~\ref{fig:spr:effects}.
The estimate for the sentences with \NACa\ is obviously imprecise and no significant differences in reading times are observed.
There is a clearer effect in the sentences with \PGCa, which is also confirmed by the significant results from the bootstrapped confidence intervals (see Table~\ref{tab:exp:spr}) and from the PB test reported above.
The \PGCa\ brings about an increased reading time, which is plausible because it is the much rarer construction (see Section~\ref{sec:corpusstudies}).
However, if it occurs in a prototypical context and with prototypical lexical material, reading times drop.
This can be seen in the downward slope in the right panel of Figure~\ref{fig:spr:effects}.
This fits into the general picture inasmuch as the construction with the lower frequency might be developing towards a more sharply defined prototype.%
\footnote{In this context, it should be remembered from Section~\ref{sec:corpusstudies} that even the \PGCd\ is much rarer that the \NACb\ (17,252 vs.\ 315,635 occurrences in the auxiliary corpus samples).}
Conversely, the \NACa\ (like the NAC in general) might be the highly frequent default which does not incur a reading time penalty, even if it is not the optimal choice in the given context and with the given lexical material.

In Section~\ref{sec:conclusion}, I take stock and summarise the contribution of the present study to the research on alternations in cognitive linguistics.
